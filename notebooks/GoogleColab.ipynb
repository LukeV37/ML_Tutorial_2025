{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de1f0e-7d91-45b2-b615-942c0b210c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/LukeV37/ML_Tutorial_2025.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9ae4a-61d3-4c8a-bc9c-83c0690131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ML_Tutorial_2025/software\n",
    "!curl -O https://www.pythia.org/download/pythia83/pythia8312.tgz\n",
    "!tar xfz pythia8312.tgz\n",
    "%cd pythia8312\n",
    "!./configure --with-python-config=/usr/bin/python3.11-config\n",
    "!make -j4\n",
    "%cd ../../notebooks\n",
    "!pip install awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the necessary pythia build paths to the python paths\n",
    "import sys\n",
    "cfg = open(\"../software/pythia8312/Makefile.inc\")  # Read necessary paths from this file\n",
    "lib = \"../software/lib\"\n",
    "for line in cfg:\n",
    "    if line.startswith(\"PREFIX_LIB=\"): lib = line[11:-1]; break  # Find build paths\n",
    "sys.path.insert(0, lib)   # Add build paths to system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d63096-67ac-40ab-8f30-7951ca58a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pythia8\n",
    "import pythia8\n",
    "\n",
    "# Import Tensor Libs\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "# Import ML Libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Plotting Libs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f783c-8271-4bb4-a3db-eb7afe1a810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_sig = pythia8.Pythia()                  # Define a pythia8.Pythia() object\n",
    "pythia_sig.readString(\"Beams:eCM = 14000.\")    # Beam energy is 14TeV\n",
    "pythia_sig.readString(\"Beams:idA = 2212\")      # Incoming particle 1 is proton\n",
    "pythia_sig.readString(\"Beams:idB = 2212\")      # Incoming particle 2 is proton\n",
    "pythia_sig.readString(\"Top:qqbar2ttbar = on\")  # Turn on all top processes\n",
    "pythia_sig.init()                              # Initialize object with user defined settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012c032-2432-4859-ad3c-a876bd2d0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_bkg = pythia8.Pythia()                  # Define a pythia8.Pythia() object\n",
    "pythia_bkg.readString(\"Beams:eCM = 14000.\")    # Beam enegery is 14TeV\n",
    "pythia_bkg.readString(\"Beams:idA = 2212\")      # Incoming particle 1 is proton\n",
    "pythia_bkg.readString(\"Beams:idB = 2212\")      # Incoming particle 2 is proton\n",
    "pythia_bkg.readString(\"WeakDoubleBoson:ffbar2WW = on\")      # Turn on all Diboson process\n",
    "pythia_bkg.init()                              # Initialize object with user defined settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0a59a-e26b-4da9-bd07-28cf9fa23811",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7eae0c-f7b2-4c66-b488-c671b8f980c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin event loop. Generate event. Skip if error.\n",
    "sig_events = []\n",
    "for iEvent in range(num_events):           # Loop through events\n",
    "    event_sig = []\n",
    "    if iEvent%5==0:\n",
    "        print(\"Generating Signal: \", iEvent, \" / \", num_events, end=\"\\r\")\n",
    "    if not pythia_sig.next(): continue     # Standard pythia syntax to trigger next event generation\n",
    "    for prt in pythia_sig.event:           # Loop through particles in each event\n",
    "        if prt.isFinal():                  # Check if particle is final state particle and store pT, eta, phi\n",
    "            event_sig.append([prt.pT(),prt.eta(),prt.phi()])\n",
    "    sig_events.append(event_sig)\n",
    "print(\"Done Generating Signal: \", num_events, \" / \", num_events)\n",
    "\n",
    "print(\"Converting to Awkward Array...\")\n",
    "sig_events = ak.Array(sig_events)\n",
    "print(\"Done Converting to Awkward Array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd407d-bdec-48a6-be9f-68959e9bb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin event loop. Generate event. Skip if error.\n",
    "bkg_events = []\n",
    "for iEvent in range(num_events):          # Loop through events\n",
    "    event_bkg = []\n",
    "    if iEvent%5==0:\n",
    "        print(\"Generating Background: \", iEvent, \" / \", num_events, end=\"\\r\")\n",
    "    if not pythia_bkg.next(): continue    # Standard pythia syntax to trigger enxt event generation\n",
    "    for prt in pythia_bkg.event:          # Loop through particles in each event\n",
    "        if prt.isFinal():                 # Chekc if particle is final stat particle and store pT, eta, phi\n",
    "            event_bkg.append([prt.pT(),prt.eta(),prt.phi()])\n",
    "    bkg_events.append(event_bkg)\n",
    "print(\"Done Generating Background: \", num_events, \" / \", num_events)\n",
    "\n",
    "print(\"Converting to Awkward Array...\")\n",
    "bkg_events = ak.Array(bkg_events)\n",
    "print(\"Done Converting to Awkward Array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a2eb9-7a67-4b5e-92dd-c42bc599d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Num Tracks per Event\")\n",
    "plt.hist(ak.num(sig_events),histtype='step',color='r',range=(0,1000),bins=30,label='sig')\n",
    "plt.hist(ak.num(bkg_events),histtype='step',color='b',range=(0,1000),bins=30,label='bkg')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255a120-b358-40dd-bfe2-d0d96083af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pT for signal and background\n",
    "plt.title(\"Particle p$\\mathregular{_{T}}$\")\n",
    "plt.hist(ak.ravel(sig_events[:,:,0]),bins=40,range=(0,250),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(bkg_events[:,:,0]),bins=40,range=(0,250),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.xlabel('GeV',loc='right')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot eta for signal and background\n",
    "plt.title(\"Particle \\u03B7\")\n",
    "plt.hist(ak.ravel(sig_events[:,:,1]),bins=30,range=(-10,10),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(bkg_events[:,:,1]),bins=30,range=(-10,10),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot phi for signal and background\n",
    "plt.title(\"Particle \\u03D5\")\n",
    "plt.hist(ak.ravel(sig_events[:,:,2]),bins=16,range=(-4,4),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(bkg_events[:,:,2]),bins=16,range=(-4,4),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98fbf9-0b03-40b0-8ca3-de846588c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_args = ak.argsort(sig_events[:,:,0], ascending=False)\n",
    "sorted_sig = sig_events[sorted_args]\n",
    "sig_labels = np.ones((len(sorted_sig),1))\n",
    "\n",
    "sorted_args = ak.argsort(bkg_events[:,:,0], ascending=False)\n",
    "sorted_bkg = bkg_events[sorted_args]\n",
    "bkg_labels = np.zeros((len(sorted_bkg),1))\n",
    "\n",
    "combined_data = ak.concatenate([sorted_sig,sorted_bkg], axis=0)\n",
    "combined_labels = ak.concatenate([sig_labels,bkg_labels], axis=0)\n",
    "\n",
    "p = np.random.permutation(len(combined_data))\n",
    "combined_data = combined_data[p]\n",
    "combined_labels = combined_labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd25bc9-3e71-4dca-ba5c-58f56f01a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_particle_cutoff = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257070d0-d89c-420f-9b19-e0deef5b0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = []\n",
    "for i, event in enumerate(combined_data):\n",
    "    if i%5==0:\n",
    "        print(\"Padding Dataset: \", i, \" / \", len(combined_data), end=\"\\r\")\n",
    "    event_data = []\n",
    "    for j in range(padding_particle_cutoff):\n",
    "        if j<len(event):\n",
    "            event_data.append(event[j])\n",
    "        else:\n",
    "            event_data.append([0,0,0])\n",
    "    padded_data.append(event_data)\n",
    "print(\"Done Padding Dataset: \", len(combined_data), \" / \", len(combined_data))\n",
    "\n",
    "print(\"Converting to Awkward Array...\")\n",
    "padded_data = ak.Array(padded_data)\n",
    "print(\"Done Converting to Awkward Array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b313a4-fe1c-4150-bb10-232f6e4c864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = combined_labels==1\n",
    "bkg = combined_labels==0\n",
    "\n",
    "# Plot pT for signal and background\n",
    "plt.title(\"Particle p$\\mathregular{_{T}}$\")\n",
    "plt.hist(ak.ravel(padded_data[sig][:,0]),bins=40,range=(0,250),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(padded_data[bkg][:,0]),bins=40,range=(0,250),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.xlabel('GeV',loc='right')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot eta for signal and background\n",
    "plt.title(\"Particle \\u03B7\")\n",
    "plt.hist(ak.ravel(padded_data[sig][:,1]),bins=40,range=(-10,10),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(padded_data[bkg][:,1]),bins=40,range=(-10,10),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot phi for signal and background\n",
    "plt.title(\"Particle \\u03D5\")\n",
    "plt.hist(ak.ravel(padded_data[sig][:,2]),bins=40,range=(-4,4),histtype='step',label='sig',color='r',density=True)\n",
    "plt.hist(ak.ravel(padded_data[bkg][:,2]),bins=40,range=(-4,4),histtype='step',label='bkg',color='b',density=True)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    def __init__(self, data, labels, device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return event, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, if not use cpu\n",
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfad1a-b202-4b25-9eec-edadb4ca0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.7*len(padded_data))\n",
    "test_split = int(0.75*len(padded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840ef74-3508-43e4-afa0-21437e4b79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ParticleDataset(padded_data[0:train_split], combined_labels[0:train_split], device)\n",
    "val_dataset = ParticleDataset(padded_data[train_split:test_split], combined_labels[train_split:test_split], device)\n",
    "test_dataset = ParticleDataset(padded_data[test_split:], combined_labels[test_split:], device)\n",
    "\n",
    "torch.save(train_dataset, \"../datasets/train_dataset.pt\")\n",
    "torch.save(val_dataset, \"../datasets/val_dataset.pt\")\n",
    "torch.save(test_dataset, \"../datasets/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df69853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"../datasets/train_dataset_700k.pt\", weights_only=False, map_location=device)\n",
    "val_dataset = torch.load(\"../datasets/val_dataset_50k.pt\", weights_only=False, map_location=device)\n",
    "test_dataset = torch.load(\"../datasets/test_dataset_250k.pt\", weights_only=False, map_location=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1024, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in train_dataloader:\n",
    "    print(\"Batch shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    padding_particle_cutoff=inputs.shape[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    A DL model with customizable layers and nodes. \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, out_dim):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        hidden_layers = [hidden_dim]*num_layers\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.gelu(layer(x))\n",
    "        x = F.sigmoid(self.layers[-1](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaae344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class that inherits from torch.nn.Module\n",
    "class DeepSets(nn.Module):\n",
    "    '''\n",
    "    A DeepSets model that performs graph level classification on a dense graph. \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.init = nn.Linear(in_dim, hidden_dim)\n",
    "        self.Messages = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.PostProcess = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.Classification = nn.Linear(hidden_dim,out_dim)\n",
    "    def forward(self, data):\n",
    "        track_embedding = F.gelu(self.init(data))\n",
    "        messages = F.gelu(self.Messages(track_embedding))\n",
    "        aggregated_message = torch.sum(messages,dim=1)\n",
    "        event_embedding = F.gelu(self.PostProcess(aggregated_message))\n",
    "        output = F.sigmoid(self.Classification(event_embedding))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46840065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class that inherits from torch.nn.Module\n",
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    An Attention model that performs set level classification on a set. \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim, num_encoders, out_dim):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.init = nn.Linear(in_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=1, dim_feedforward=hidden_dim,dropout=0, batch_first=True)        # Define linear transformation 1\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "        self.Classification = nn.Linear(hidden_dim,out_dim)\n",
    "    def forward(self, data):\n",
    "        embedding = F.gelu(self.init(data))\n",
    "        embedding = F.gelu(self.transformer_encoder(embedding))\n",
    "        embedding = torch.mean(embedding,dim=1)\n",
    "        output = F.sigmoid(self.Classification(embedding))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, optimizer, loss_fn, train_dataloader, val_dataloader, lr_step, epochs=20):\n",
    "\n",
    "    history = {'train_loss':[],'test_loss':[]}     # Define history dictionary\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=0.1)\n",
    "    # Loop through epoches\n",
    "    for e in range(epochs):\n",
    "        for X_train, y_train in train_dataloader:\n",
    "            # Train Model\n",
    "            model.train()                        # Switch model to training mode\n",
    "            optimizer.zero_grad()                # Reset the optimizers gradients\n",
    "            y_pred = model(X_train)             # Get the model prediction\n",
    "            loss = loss_fn(y_pred, y_train)     # Evaluate loss function\n",
    "            loss.backward()                      # Backward propogation\n",
    "            optimizer.step()                     # Gradient Descent\n",
    "        for X_val, y_val in val_dataloader:\n",
    "            # Validate Model\n",
    "            model.eval()\n",
    "            y_pred = model(X_val)    # Get model output on test data\n",
    "            test_loss = loss_fn(y_pred,y_val)   # Evaluate loss on test preditions\n",
    "        \n",
    "        history['train_loss'].append(loss.detach().cpu().numpy())        # Append train loss to history (detach and convert to numpy array)\n",
    "        history['test_loss'].append(test_loss.detach().cpu().numpy())    # Append test loss to history (detach and convert to numpy array)\n",
    "        if (e+1)%1==0:\n",
    "            print('Epoch:',e+1,'\\tTrain Loss:',round(float(loss),4),'\\tTest Loss:',round(float(test_loss),4))\n",
    "        scheduler.step()\n",
    "        if (e+1)%lr_step==0:\n",
    "            print(\"\\tReducing Learning Rate!\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04323f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "MLP = MultiLayerPerceptron(in_dim=3*padding_particle_cutoff,hidden_dim=164, num_layers=2, out_dim=1)    # Declare model using NeuralNet Class\n",
    "MLP.to(device)                                       # Put model on device (cpu or gpu)\n",
    "print(MLP)                                           # Print layers in model\n",
    "\n",
    "# Calculate and print trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in MLP.parameters() if p.requires_grad)\n",
    "print(\"Trainable Parameters: \", pytorch_total_params,\"\\n\")\n",
    "\n",
    "# Declare optimizer and loss function\n",
    "optimizer = optim.AdamW(MLP.parameters(), lr=1e-6)  # Adam = Adaptive Moment Estimation, lr = learning rate\n",
    "loss_fn = nn.BCELoss()                                 # BCE = Binary Cross Entropy, used for binary classification\n",
    "\n",
    "#Train Model\n",
    "history = train(MLP, optimizer, loss_fn, train_dataloader, val_dataloader, lr_step=15, epochs=30)                                             # Train the model!\n",
    "\n",
    "# Plot Training History\n",
    "plt.plot(history['train_loss'],label='train')\n",
    "plt.plot(history['test_loss'],label='test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "torch.save(MLP, \"MLP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "GNN = DeepSets(in_dim=3,hidden_dim=222,out_dim=1)    # Declare model using NeuralNet Class\n",
    "GNN.to(device)                                       # Put model on device (cpu or gpu)\n",
    "print(GNN)                                           # Print layers in model\n",
    "\n",
    "# Calculate and print trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in GNN.parameters() if p.requires_grad)\n",
    "print(\"Trainable Parameters: \", pytorch_total_params,\"\\n\")\n",
    "\n",
    "# Declare optimizer and loss function\n",
    "optimizer = optim.AdamW(GNN.parameters(), lr=1e-6)  # Adam = Adaptive Moment Estimation, lr = learning rate\n",
    "loss_fn = nn.BCELoss()                                 # BCE = Binary Cross Entropy, used for binary classification\n",
    "\n",
    "#Train Model\n",
    "history = train(GNN, optimizer, loss_fn, train_dataloader, val_dataloader, lr_step=15, epochs=30)                                             # Train the model!\n",
    "\n",
    "# Plot Training History\n",
    "plt.plot(history['train_loss'],label='train')\n",
    "plt.plot(history['test_loss'],label='test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "torch.save(GNN, \"GNN.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "Transformer = TransformerEncoder(in_dim=3,hidden_dim=90,num_encoders=2,out_dim=1)    # Declare model using NeuralNet Class\n",
    "Transformer.to(device)                                       # Put model on device (cpu or gpu)\n",
    "print(Transformer)                                           # Print layers in model\n",
    "\n",
    "# Calculate and print trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in Transformer.parameters() if p.requires_grad)\n",
    "print(\"Trainable Parameters: \", pytorch_total_params,\"\\n\")\n",
    "\n",
    "# Declare optimizer and loss function\n",
    "optimizer = optim.AdamW(Transformer.parameters(), lr=1e-5)  # Adam = Adaptive Moment Estimation, lr = learning rate\n",
    "loss_fn = nn.BCELoss()                                 # BCE = Binary Cross Entropy, used for binary classification\n",
    "\n",
    "#Train Model\n",
    "history = train(Transformer, optimizer, loss_fn, train_dataloader, val_dataloader, lr_step=15, epochs=30)                                             # Train the model!\n",
    "\n",
    "# Plot Training History\n",
    "plt.plot(history['train_loss'],label='train')\n",
    "plt.plot(history['test_loss'],label='test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "torch.save(Transformer, \"Transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = torch.load(\"MLP.pt\", weights_only=False, map_location=device)\n",
    "GNN = torch.load(\"GNN.pt\", weights_only=False, map_location=device)\n",
    "Transformer = torch.load(\"Transformer.pt\", weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traditional ROC curve\n",
    "def roc(y_pred,y_true):    \n",
    "    sig_eff = []\n",
    "    bkg_eff = []\n",
    "    \n",
    "    sig = y_true==1\n",
    "    bkg = y_true==0\n",
    "    \n",
    "    thresholds = np.linspace(0,1,100)\n",
    "    \n",
    "    # Iterate over thresholds and calculate sig and bkg efficiency\n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))   # Sum over sig predictions > threshold and divide by total number of true sig instances \n",
    "        bkg_eff.append(((y_pred[bkg] < threshold).sum()  / y_true[bkg].shape[0]))  # Sum over bkg predictions < threshold and divide by total number of true bkg instances \n",
    "        \n",
    "    return np.array(sig_eff), np.array(bkg_eff), thresholds\n",
    "\n",
    "# Define ATLAS Style ROC curve\n",
    "def ATLAS_roc(y_pred,y_true):\n",
    "    sig_eff = []\n",
    "    bkg_eff = []\n",
    "    \n",
    "    sig = y_true==1\n",
    "    bkg = y_true==0\n",
    "    \n",
    "    thresholds = np.linspace(0,1,1000)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))\n",
    "        bkg_eff.append(1-((y_pred[bkg] < threshold).sum()  / y_true[bkg].shape[0]))\n",
    "        \n",
    "    bkg_rej = [1/x for x in bkg_eff]  # ATLAS inverts bkg eff and uses bkg rejection instead\n",
    "    return np.array(sig_eff), np.array(bkg_rej), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecafce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loss_fn, test_dataloader):\n",
    "    # Get Models predictions\n",
    "    prediction = []\n",
    "    truth = []\n",
    "    test_loss = []\n",
    "    for X_test, y_test in test_dataloader:\n",
    "        model.eval()\n",
    "        y_pred = model(X_test)    # Get model output on test data\n",
    "        loss = loss_fn(y_pred,y_test)\n",
    "        test_loss.append(loss.detach().cpu().numpy())   # Evaluate loss on test preditions\n",
    "        prediction.append(y_pred.detach().cpu().numpy())\n",
    "        truth.append(y_test.detach().cpu().numpy())\n",
    "\n",
    "    prediction = np.array(ak.ravel(prediction))\n",
    "    truth = np.array(ak.ravel(truth))\n",
    "    \n",
    "    # Find indices of sig and bkg labels\n",
    "    sig = np.where(truth==1)\n",
    "    bkg = np.where(truth==0)\n",
    "    \n",
    "    eff_sig, eff_bkg, thresh = roc(prediction,truth)\n",
    "    \n",
    "    WPs = [25,50,75]\n",
    "    cuts = []\n",
    "    for WP in WPs:\n",
    "        mask = eff_sig>(WP/100)\n",
    "        idx = len(eff_sig[mask])-1\n",
    "        cut = thresh[idx]\n",
    "        cuts.append(cut)\n",
    "\n",
    "    # Plot Model Predictions\n",
    "    plt.title(model.__class__.__name__+\" Predictions\")\n",
    "    plt.hist(prediction[sig],histtype='step',color='r',label=\"sig\",bins=40)\n",
    "    plt.hist(prediction[bkg],histtype='step',color='b',label=\"bkg\",bins=40)\n",
    "    colors=['c','m','y']\n",
    "    labels=[str(x)+\"% WP\" for x in WPs]\n",
    "    for i, cut in enumerate(cuts):\n",
    "        plt.axvline(cut,linestyle='--',color=colors[i%3],label=labels[i])\n",
    "    plt.xlabel(\"Model Score\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    #plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return prediction, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "MLP_pred, MLP_true = eval_model(MLP, loss_fn, test_dataloader)\n",
    "GNN_pred, GNN_true = eval_model(GNN, loss_fn, test_dataloader)\n",
    "Trans_pred, Trans_true = eval_model(Transformer, loss_fn, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tradiation ROC Curve\n",
    "MLP_eff_sig, MLP_eff_bkg, MLP_thresh = roc(MLP_pred,MLP_true)\n",
    "GNN_eff_sig, GNN_eff_bkg, GNN_thresh = roc(GNN_pred,GNN_true)\n",
    "Trans_eff_sig, Trans_eff_bkg, Trans_thresh = roc(Trans_pred,Trans_true)\n",
    "\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot(MLP_eff_sig,MLP_eff_bkg,color='r',label=\"MLP\")\n",
    "plt.plot(GNN_eff_sig,GNN_eff_bkg,color='g',label=\"GNN\")\n",
    "plt.plot(Trans_eff_sig,Trans_eff_bkg,color='b',label=\"Transformer\")\n",
    "plt.plot([1,0],'--',color='k',label=\"Random Model\")\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Background Efficiency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot ATLAS Style ROC Curve\n",
    "MLP_eff_sig, MLP_eff_bkg, MLP_thresh = ATLAS_roc(MLP_pred,MLP_true)\n",
    "GNN_eff_sig, GNN_eff_bkg, GNN_thresh = ATLAS_roc(GNN_pred,GNN_true)\n",
    "Trans_eff_sig, Trans_eff_bkg, Trans_thresh = ATLAS_roc(Trans_pred,Trans_true)\n",
    "\n",
    "plt.title(\"ATLAS ROC Curve\")\n",
    "plt.plot(MLP_eff_sig,MLP_eff_bkg,color='r',label=\"MLP\")\n",
    "plt.plot(GNN_eff_sig,GNN_eff_bkg,color='g',label=\"GNN\")\n",
    "plt.plot(Trans_eff_sig,Trans_eff_bkg,color='b',label=\"Transformer\")\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True,which='both')\n",
    "plt.xlim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"ATLAS ROC Curve\")\n",
    "plt.plot(MLP_eff_sig,MLP_eff_bkg,color='r',label=\"MLP\")\n",
    "plt.plot(GNN_eff_sig,GNN_eff_bkg,color='g',label=\"GNN\")\n",
    "plt.plot(Trans_eff_sig,Trans_eff_bkg,color='b',label=\"Transformer\")\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True,which='both')\n",
    "plt.xlim([0, 0.2])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"ATLAS ROC Curve\")\n",
    "plt.plot(MLP_eff_sig,MLP_eff_bkg,color='r',label=\"MLP\")\n",
    "plt.plot(GNN_eff_sig,GNN_eff_bkg,color='g',label=\"GNN\")\n",
    "plt.plot(Trans_eff_sig,Trans_eff_bkg,color='b',label=\"Transformer\")\n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True,which='both')\n",
    "plt.xlim([0.7, 1])\n",
    "plt.ylim([0, 18])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235254b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Tutorial_2025",
   "language": "python",
   "name": "ml_tutorial_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
